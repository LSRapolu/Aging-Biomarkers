{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+2cbElEC5fAJYIbkymWJp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Modeling Trials**"],"metadata":{"id":"QyQyLORe-JYC"}},{"cell_type":"markdown","source":["###**Modeling without optimization discarded due to time constraints**"],"metadata":{"id":"kE0GSmC9-WVh"}},{"cell_type":"code","source":["# Model Training and Optimization - 1st Iteration\n","# ============================================================================\n","\n","def train_optimized_model(model_name, objective_func, n_trials=50):\n","    \"\"\"Train and optimize a model using Optuna\"\"\"\n","    print(f\"\\nüîÑ Optimizing {model_name}...\")\n","    print(f\"   Running {n_trials} optimization trials...\")\n","\n","    # Create optimization study\n","    study = optuna.create_study(\n","        direction='minimize',\n","        sampler=TPESampler(seed=42),\n","        study_name=f\"{model_name}_optimization\"\n","    )\n","\n","    # Progress callback\n","    def progress_callback(study, trial):\n","        if trial.number % 10 == 0:\n","            print(f\"   Trial {trial.number}: Best MSE = {study.best_value:.4f}\")\n","\n","    # Run optimization\n","    start_time = time.time()\n","    study.optimize(objective_func, n_trials=n_trials, callbacks=[progress_callback])\n","    optimization_time = time.time() - start_time\n","\n","    print(f\"‚úÖ {model_name} optimization completed in {optimization_time:.1f} seconds\")\n","    print(f\"üèÜ Best CV MSE: {study.best_value:.4f}\")\n","\n","    return study\n","\n","# Train XGBoost\n","print(f\"\\n{'='*60}\")\n","print(\"üöÄ TRAINING XGBOOST MODEL\")\n","print(f\"{'='*60}\")\n","\n","xgb_study = train_optimized_model(\"XGBoost\", objective_xgboost, n_trials=50)\n","\n","# Train final XGBoost model with best parameters\n","print(\"üîß Training final XGBoost model...\")\n","best_xgb_params = xgb_study.best_params.copy()\n","\n","if gpu_config['xgboost_gpu']:\n","    best_xgb_params.update({'tree_method': 'gpu_hist', 'gpu_id': 0})\n","else:\n","    best_xgb_params.update({'tree_method': 'hist', 'n_jobs': -1})\n","\n","final_xgb = xgb.XGBRegressor(**best_xgb_params)\n","final_xgb.fit(X_train_processed, y_train)\n","\n","# Evaluate XGBoost\n","xgb_pred = final_xgb.predict(X_test_processed)\n","xgb_mse = mean_squared_error(y_test, xgb_pred)\n","xgb_mae = mean_absolute_error(y_test, xgb_pred)\n","xgb_r2 = r2_score(y_test, xgb_pred)\n","\n","print(f\"üìä XGBoost Final Results:\")\n","print(f\"   Test MSE: {xgb_mse:.4f}\")\n","print(f\"   Test MAE: {xgb_mae:.4f}\")\n","print(f\"   Test R¬≤:  {xgb_r2:.4f}\")\n","\n","# Train LightGBM\n","print(f\"\\n{'='*60}\")\n","print(\"üöÄ TRAINING LIGHTGBM MODEL\")\n","print(f\"{'='*60}\")\n","\n","lgb_study = train_optimized_model(\"LightGBM\", objective_lightgbm, n_trials=50)\n","\n","# Train final LightGBM model\n","print(\"üîß Training final LightGBM model...\")\n","final_lgb = lgb.LGBMRegressor(**lgb_study.best_params)\n","final_lgb.fit(X_train_processed, y_train)\n","\n","# Evaluate LightGBM\n","lgb_pred = final_lgb.predict(X_test_processed)\n","lgb_mse = mean_squared_error(y_test, lgb_pred)\n","lgb_mae = mean_absolute_error(y_test, lgb_pred)\n","lgb_r2 = r2_score(y_test, lgb_pred)\n","\n","print(f\"üìä LightGBM Final Results:\")\n","print(f\"   Test MSE: {lgb_mse:.4f}\")\n","print(f\"   Test MAE: {lgb_mae:.4f}\")\n","print(f\"   Test R¬≤:  {lgb_r2:.4f}\")\n","\n","# Train CatBoost\n","print(f\"\\n{'='*60}\")\n","print(\"üöÄ TRAINING CATBOOST MODEL\")\n","print(f\"{'='*60}\")\n","\n","cb_study = train_optimized_model(\"CatBoost\", objective_catboost, n_trials=40)\n","\n","# Train final CatBoost model\n","print(\"üîß Training final CatBoost model...\")\n","best_cb_params = cb_study.best_params.copy()\n","\n","if gpu_config['catboost_gpu']:\n","    best_cb_params['task_type'] = 'GPU'\n","else:\n","    best_cb_params['task_type'] = 'CPU'\n","    best_cb_params['thread_count'] = -1\n","\n","final_cb = cb.CatBoostRegressor(**best_cb_params)\n","final_cb.fit(X_train_processed, y_train)\n","\n","# Evaluate CatBoost\n","cb_pred = final_cb.predict(X_test_processed)\n","cb_mse = mean_squared_error(y_test, cb_pred)\n","cb_mae = mean_absolute_error(y_test, cb_pred)\n","cb_r2 = r2_score(y_test, cb_pred)\n","\n","print(f\"üìä CatBoost Final Results:\")\n","print(f\"   Test MSE: {cb_mse:.4f}\")\n","print(f\"   Test MAE: {cb_mae:.4f}\")\n","print(f\"   Test R¬≤:  {cb_r2:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"h_18BiYz5w_4","executionInfo":{"status":"error","timestamp":1749074165470,"user_tz":240,"elapsed":587309,"user":{"displayName":"Lakshmi Sreya Rapolu","userId":"14242897370658562536"}},"outputId":"ab69b260-8850-4b3d-ed40-4de1e2c1af29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","üöÄ TRAINING XGBOOST MODEL\n","============================================================\n","\n","üîÑ Optimizing XGBoost...\n","   Running 50 optimization trials...\n","   Trial 0: Best MSE = 30.8204\n"]},{"output_type":"stream","name":"stderr","text":["[W 2025-06-04 21:56:05,274] Trial 10 failed with parameters: {'n_estimators': 1446, 'max_depth': 4, 'learning_rate': 0.06162202818095362, 'subsample': 0.7044904689037048, 'colsample_bytree': 0.8051010020460994, 'reg_alpha': 1.963435505233873, 'reg_lambda': 4.857589904335454, 'min_child_weight': 7} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n","    value_or_values = func(trial)\n","                      ^^^^^^^^^^^\n","  File \"<ipython-input-20-87c2f878690b>\", line 29, in objective_xgboost\n","    cv_scores = cross_val_score(\n","                ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n","    return func(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 684, in cross_val_score\n","    cv_results = cross_validate(\n","                 ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n","    return func(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 411, in cross_validate\n","    results = parallel(\n","              ^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 77, in __call__\n","    return super().__call__(iterable_with_config)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1986, in __call__\n","    return output if self.return_generator else list(output)\n","                                                ^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n","    res = func(*args, **kwargs)\n","          ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n","    return self.function(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","           ^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\", line 1170, in fit\n","    self._Booster = train(\n","                    ^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 726, in inner_f\n","    return func(**kwargs)\n","           ^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/xgboost/training.py\", line 181, in train\n","    bst.update(dtrain, iteration=i, fobj=obj)\n","  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 2101, in update\n","    _LIB.XGBoosterUpdateOneIter(\n","KeyboardInterrupt\n","[W 2025-06-04 21:56:05,277] Trial 10 failed with value None.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-38a230e150c4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'='*60}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mxgb_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_optimized_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"XGBoost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective_xgboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Train final XGBoost model with best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-38a230e150c4>\u001b[0m in \u001b[0;36mtrain_optimized_model\u001b[0;34m(model_name, objective_func, n_trials)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Run optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprogress_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0moptimization_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-87c2f878690b>\u001b[0m in \u001b[0;36mobjective_xgboost\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Use cross-validation for robust evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     cv_scores = cross_val_score(\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1171\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Model Training and Optimization - 2nd Iteration\n","# ============================================================================\n","# OPTIMIZED FAST MODEL TRAINING (Space + Speed + GPU Optimized)\n","# ============================================================================\n","\n","print(\"üöÄ Starting optimized fast training...\")\n","print(\"‚ö° Optimizations: Reduced trials, GPU-first, memory efficient\")\n","\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ============================================================================\n","# GPU Detection and Verification\n","# ============================================================================\n","\n","def verify_gpu_acceleration():\n","    \"\"\"Verify and force GPU usage where possible\"\"\"\n","    gpu_status = {'xgboost': False, 'catboost': False}\n","\n","    # Test XGBoost GPU\n","    try:\n","        test_xgb = xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0, n_estimators=5)\n","        test_xgb.fit(X_train_processed[:20], y_train[:20])\n","        gpu_status['xgboost'] = True\n","        print(\"‚úÖ XGBoost GPU verified\")\n","        del test_xgb\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  XGBoost GPU failed: {str(e)[:50]}...\")\n","\n","    # Test CatBoost GPU\n","    try:\n","        test_cb = cb.CatBoostRegressor(task_type='GPU', iterations=5, verbose=False)\n","        test_cb.fit(X_train_processed[:20], y_train[:20])\n","        gpu_status['catboost'] = True\n","        print(\"‚úÖ CatBoost GPU verified\")\n","        del test_cb\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  CatBoost GPU failed: {str(e)[:50]}...\")\n","\n","    gc.collect()\n","    return gpu_status\n","\n","gpu_status = verify_gpu_acceleration()\n","\n","# ============================================================================\n","# Optimized Objective Functions (Reduced Search Space)\n","# ============================================================================\n","\n","def fast_xgboost_objective(trial):\n","    \"\"\"Streamlined XGBoost optimization\"\"\"\n","    params = {\n","        'n_estimators': trial.suggest_categorical('n_estimators', [600, 800, 1000]),\n","        'max_depth': trial.suggest_int('max_depth', 5, 8),\n","        'learning_rate': trial.suggest_categorical('learning_rate', [0.03, 0.05, 0.07]),\n","        'subsample': trial.suggest_categorical('subsample', [0.8, 0.85, 0.9]),\n","        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.8, 0.85, 0.9]),\n","        'reg_alpha': trial.suggest_categorical('reg_alpha', [0.5, 1.0, 2.0]),\n","        'reg_lambda': trial.suggest_categorical('reg_lambda', [1.0, 2.0, 3.0]),\n","        'random_state': 42\n","    }\n","\n","    # Force GPU usage if available\n","    if gpu_status['xgboost']:\n","        params.update({'tree_method': 'gpu_hist', 'gpu_id': 0})\n","    else:\n","        params.update({'tree_method': 'hist', 'n_jobs': -1})\n","\n","    model = xgb.XGBRegressor(**params)\n","\n","    # Fast 3-fold CV instead of 5-fold\n","    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=3,\n","                               scoring='neg_mean_squared_error', n_jobs=1)\n","    return -cv_scores.mean()\n","\n","def fast_lightgbm_objective(trial):\n","    \"\"\"Streamlined LightGBM optimization\"\"\"\n","    params = {\n","        'n_estimators': trial.suggest_categorical('n_estimators', [600, 800, 1000]),\n","        'max_depth': trial.suggest_int('max_depth', 5, 8),\n","        'learning_rate': trial.suggest_categorical('learning_rate', [0.03, 0.05, 0.07]),\n","        'num_leaves': trial.suggest_categorical('num_leaves', [40, 60, 80]),\n","        'subsample': trial.suggest_categorical('subsample', [0.8, 0.85, 0.9]),\n","        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.8, 0.85, 0.9]),\n","        'reg_alpha': trial.suggest_categorical('reg_alpha', [0.5, 1.0, 2.0]),\n","        'reg_lambda': trial.suggest_categorical('reg_lambda', [1.0, 2.0, 3.0]),\n","        'random_state': 42,\n","        'verbose': -1,\n","        'n_jobs': -1\n","    }\n","\n","    model = lgb.LGBMRegressor(**params)\n","    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=3,\n","                               scoring='neg_mean_squared_error', n_jobs=1)\n","    return -cv_scores.mean()\n","\n","def fast_catboost_objective(trial):\n","    \"\"\"Streamlined CatBoost optimization\"\"\"\n","    params = {\n","        'iterations': trial.suggest_categorical('iterations', [600, 800, 1000]),\n","        'depth': trial.suggest_int('depth', 5, 8),\n","        'learning_rate': trial.suggest_categorical('learning_rate', [0.03, 0.05, 0.07]),\n","        'l2_leaf_reg': trial.suggest_categorical('l2_leaf_reg', [2, 3, 5]),\n","        'random_seed': 42,\n","        'verbose': False\n","    }\n","\n","    # Force GPU if available\n","    if gpu_status['catboost']:\n","        params['task_type'] = 'GPU'\n","    else:\n","        params.update({'task_type': 'CPU', 'thread_count': -1})\n","\n","    model = cb.CatBoostRegressor(**params)\n","    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=3,\n","                               scoring='neg_mean_squared_error', n_jobs=1)\n","    return -cv_scores.mean()\n","\n","# ============================================================================\n","# Fast Training Function (Reduced Trials)\n","# ============================================================================\n","\n","def train_fast_model(model_name, objective_func, n_trials=20):\n","    \"\"\"Fast model training with reduced trials\"\"\"\n","    print(f\"\\nüîÑ Fast optimization: {model_name} ({n_trials} trials)\")\n","\n","    # Create study with reduced storage for speed\n","    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n","\n","    # Progress every 5 trials instead of 10\n","    def fast_callback(study, trial):\n","        if trial.number % 5 == 0 and trial.number > 0:\n","            print(f\"   Trial {trial.number}: Best MSE = {study.best_value:.4f}\")\n","\n","    start_time = time.time()\n","    study.optimize(objective_func, n_trials=n_trials, callbacks=[fast_callback])\n","    elapsed = time.time() - start_time\n","\n","    print(f\"‚úÖ {model_name} optimized in {elapsed:.1f}s (Best: {study.best_value:.4f})\")\n","    return study\n","\n","# ============================================================================\n","# XGBoost Fast Training\n","# ============================================================================\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üöÄ FAST XGBOOST TRAINING\")\n","print(f\"{'='*50}\")\n","\n","xgb_study = train_fast_model(\"XGBoost\", fast_xgboost_objective, n_trials=20)\n","\n","print(\"üîß Training final XGBoost...\")\n","best_xgb_params = xgb_study.best_params.copy()\n","\n","if gpu_status['xgboost']:\n","    best_xgb_params.update({'tree_method': 'gpu_hist', 'gpu_id': 0})\n","    print(\"   Using GPU acceleration\")\n","else:\n","    best_xgb_params.update({'tree_method': 'hist', 'n_jobs': -1})\n","    print(\"   Using CPU (GPU unavailable)\")\n","\n","start_time = time.time()\n","final_xgb = xgb.XGBRegressor(**best_xgb_params)\n","final_xgb.fit(X_train_processed, y_train)\n","train_time = time.time() - start_time\n","\n","xgb_pred = final_xgb.predict(X_test_processed)\n","xgb_mse = mean_squared_error(y_test, xgb_pred)\n","xgb_mae = mean_absolute_error(y_test, xgb_pred)\n","xgb_r2 = r2_score(y_test, xgb_pred)\n","\n","print(f\"üìä XGBoost Results (trained in {train_time:.1f}s):\")\n","print(f\"   MSE: {xgb_mse:.4f} {'üéØ' if 2.0 <= xgb_mse <= 3.0 else '‚ùå'}\")\n","print(f\"   MAE: {xgb_mae:.4f}\")\n","print(f\"   R¬≤:  {xgb_r2:.4f}\")\n","\n","# Memory cleanup\n","del xgb_study\n","gc.collect()\n","\n","# ============================================================================\n","# LightGBM Fast Training\n","# ============================================================================\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üöÄ FAST LIGHTGBM TRAINING\")\n","print(f\"{'='*50}\")\n","\n","lgb_study = train_fast_model(\"LightGBM\", fast_lightgbm_objective, n_trials=20)\n","\n","print(\"üîß Training final LightGBM...\")\n","start_time = time.time()\n","final_lgb = lgb.LGBMRegressor(**lgb_study.best_params)\n","final_lgb.fit(X_train_processed, y_train)\n","train_time = time.time() - start_time\n","\n","lgb_pred = final_lgb.predict(X_test_processed)\n","lgb_mse = mean_squared_error(y_test, lgb_pred)\n","lgb_mae = mean_absolute_error(y_test, lgb_pred)\n","lgb_r2 = r2_score(y_test, lgb_pred)\n","\n","print(f\"üìä LightGBM Results (trained in {train_time:.1f}s):\")\n","print(f\"   MSE: {lgb_mse:.4f} {'üéØ' if 2.0 <= lgb_mse <= 3.0 else '‚ùå'}\")\n","print(f\"   MAE: {lgb_mae:.4f}\")\n","print(f\"   R¬≤:  {lgb_r2:.4f}\")\n","\n","# Memory cleanup\n","del lgb_study\n","gc.collect()\n","\n","# ============================================================================\n","# CatBoost Fast Training\n","# ============================================================================\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üöÄ FAST CATBOOST TRAINING\")\n","print(f\"{'='*50}\")\n","\n","cb_study = train_fast_model(\"CatBoost\", fast_catboost_objective, n_trials=15)  # Even fewer for CatBoost\n","\n","print(\"üîß Training final CatBoost...\")\n","best_cb_params = cb_study.best_params.copy()\n","\n","if gpu_status['catboost']:\n","    best_cb_params['task_type'] = 'GPU'\n","    print(\"   Using GPU acceleration\")\n","else:\n","    best_cb_params.update({'task_type': 'CPU', 'thread_count': -1})\n","    print(\"   Using CPU (GPU unavailable)\")\n","\n","start_time = time.time()\n","final_cb = cb.CatBoostRegressor(**best_cb_params)\n","final_cb.fit(X_train_processed, y_train)\n","train_time = time.time() - start_time\n","\n","cb_pred = final_cb.predict(X_test_processed)\n","cb_mse = mean_squared_error(y_test, cb_pred)\n","cb_mae = mean_absolute_error(y_test, cb_pred)\n","cb_r2 = r2_score(y_test, cb_pred)\n","\n","print(f\"üìä CatBoost Results (trained in {train_time:.1f}s):\")\n","print(f\"   MSE: {cb_mse:.4f} {'üéØ' if 2.0 <= cb_mse <= 3.0 else '‚ùå'}\")\n","print(f\"   MAE: {cb_mae:.4f}\")\n","print(f\"   R¬≤:  {cb_r2:.4f}\")\n","\n","# Memory cleanup\n","del cb_study\n","gc.collect()\n","\n","# ============================================================================\n","# Fast Ensemble\n","# ============================================================================\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üéØ FAST ENSEMBLE\")\n","print(f\"{'='*50}\")\n","\n","# Simple performance-based weighting\n","models_mse = {'XGBoost': xgb_mse, 'LightGBM': lgb_mse, 'CatBoost': cb_mse}\n","total_inv_mse = sum(1/mse for mse in models_mse.values())\n","weights = {name: (1/mse)/total_inv_mse for name, mse in models_mse.items()}\n","\n","print(\"üìä Ensemble weights:\")\n","for name, weight in weights.items():\n","    print(f\"   {name}: {weight:.3f}\")\n","\n","# Create ensemble\n","ensemble_pred = (weights['XGBoost'] * xgb_pred +\n","                weights['LightGBM'] * lgb_pred +\n","                weights['CatBoost'] * cb_pred)\n","\n","ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n","ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n","ensemble_r2 = r2_score(y_test, ensemble_pred)\n","\n","print(f\"\\nüìä Ensemble Results:\")\n","print(f\"   MSE: {ensemble_mse:.4f} {'üéØ' if 2.0 <= ensemble_mse <= 3.0 else '‚ùå'}\")\n","print(f\"   MAE: {ensemble_mae:.4f}\")\n","print(f\"   R¬≤:  {ensemble_r2:.4f}\")\n","\n","# ============================================================================\n","# Final Summary\n","# ============================================================================\n","\n","all_results = {\n","    'XGBoost': xgb_mse,\n","    'LightGBM': lgb_mse,\n","    'CatBoost': cb_mse,\n","    'Ensemble': ensemble_mse\n","}\n","\n","best_model = min(all_results.keys(), key=lambda k: all_results[k])\n","best_mse = all_results[best_model]\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üèÜ FAST TRAINING SUMMARY\")\n","print(f\"{'='*50}\")\n","\n","print(\"üìä All Results:\")\n","for model, mse in sorted(all_results.items(), key=lambda x: x[1]):\n","    status = \"üéØ TARGET!\" if 2.0 <= mse <= 3.0 else \"‚ùå Miss\"\n","    print(f\"   {model:10}: {mse:.4f} {status}\")\n","\n","print(f\"\\nüèÜ Best: {best_model} ({best_mse:.4f})\")\n","print(f\"üéØ Target: {'‚úÖ ACHIEVED' if 2.0 <= best_mse <= 3.0 else '‚ùå MISSED'}\")\n","print(f\"‚ö° Total trials: 55 (vs 140 in full optimization)\")\n","print(f\"üñ•Ô∏è  GPU used: {'‚úÖ YES' if any(gpu_status.values()) else '‚ùå NO'}\")\n","\n","# Quick save\n","try:\n","    fast_results = {\n","        'models': {'xgb': final_xgb, 'lgb': final_lgb, 'cb': final_cb},\n","        'predictions': {'xgb': xgb_pred, 'lgb': lgb_pred, 'cb': cb_pred, 'ensemble': ensemble_pred},\n","        'results': all_results,\n","        'best_model': best_model,\n","        'best_mse': best_mse,\n","        'y_test': y_test\n","    }\n","    joblib.dump(fast_results, \"/content/fast_training_results.joblib\")\n","    print(f\"üíæ Results saved to: /content/fast_training_results.joblib\")\n","except:\n","    print(f\"‚ö†Ô∏è  Save failed\")\n","\n","print(f\"\\nüöÄ Fast training complete! Ready for evaluation.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvW_sBc18fAP","outputId":"422dbb59-aaf2-43d1-c0db-d312313880ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting optimized fast training...\n","‚ö° Optimizations: Reduced trials, GPU-first, memory efficient\n","‚úÖ XGBoost GPU verified\n","‚úÖ CatBoost GPU verified\n","\n","==================================================\n","üöÄ FAST XGBOOST TRAINING\n","==================================================\n","\n","üîÑ Fast optimization: XGBoost (20 trials)\n","   Trial 5: Best MSE = 31.5478\n","   Trial 10: Best MSE = 30.3644\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vD5yvtbn-Uan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# OPTIMIZED FAST MODEL TRAINING (Space + Speed + GPU Optimized)\n","# ============================================================================\n","\n","print(\"üöÄ Starting optimized fast training...\")\n","print(\"‚ö° Optimizations: Reduced trials, GPU-first, memory efficient\")\n","\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ============================================================================\n","# GPU Detection and Verification\n","# ============================================================================\n","\n","def verify_gpu_acceleration():\n","    \"\"\"Verify and force GPU usage where possible\"\"\"\n","    gpu_status = {'xgboost': False, 'catboost': False}\n","\n","    # Test XGBoost GPU\n","    try:\n","        test_xgb = xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0, n_estimators=5)\n","        test_xgb.fit(X_train_processed[:20], y_train[:20])\n","        gpu_status['xgboost'] = True\n","        print(\"‚úÖ XGBoost GPU verified\")\n","        del test_xgb\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  XGBoost GPU failed: {str(e)[:50]}...\")\n","\n","    # Test CatBoost GPU\n","    try:\n","        test_cb = cb.CatBoostRegressor(task_type='GPU', iterations=5, verbose=False)\n","        test_cb.fit(X_train_processed[:20], y_train[:20])\n","        gpu_status['catboost'] = True\n","        print(\"‚úÖ CatBoost GPU verified\")\n","        del test_cb\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  CatBoost GPU failed: {str(e)[:50]}...\")\n","\n","    gc.collect()\n","    return gpu_status\n","\n","gpu_status = verify_gpu_acceleration()\n","\n","# ============================================================================\n","# Optimized Objective Functions (Reduced Search Space)\n","# ============================================================================\n","\n","def fast_xgboost_objective(trial):\n","    \"\"\"Streamlined XGBoost optimization\"\"\"\n","    params = {\n","        'n_estimators': trial.suggest_categorical('n_estimators', [600, 800, 1000]),\n","        'max_depth': trial.suggest_int('max_depth', 5, 8),\n","        'learning_rate': trial.suggest_categorical('learning_rate', [0.03, 0.05, 0.07]),\n","        'subsample': trial.suggest_categorical('subsample', [0.8, 0.85, 0.9]),\n","        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.8, 0.85, 0.9]),\n","        'reg_alpha': trial.suggest_categorical('reg_alpha', [0.5, 1.0, 2.0]),\n","        'reg_lambda': trial.suggest_categorical('reg_lambda', [1.0, 2.0, 3.0]),\n","        'random_state': 42\n","    }\n","\n","    # Force GPU usage if available\n","    if gpu_status['xgboost']:\n","        params.update({'tree_method': 'gpu_hist', 'gpu_id': 0})\n","    else:\n","        params.update({'tree_method': 'hist', 'n_jobs': -1})\n","\n","    model = xgb.XGBRegressor(**params)\n","\n","    # Fast 3-fold CV instead of 5-fold\n","    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=3,\n","                               scoring='neg_mean_squared_error', n_jobs=1)\n","    return -cv_scores.mean()\n","\n","def fast_lightgbm_objective(trial):\n","    \"\"\"Streamlined LightGBM optimization\"\"\"\n","    params = {\n","        'n_estimators': trial.suggest_categorical('n_estimators', [600, 800, 1000]),\n","        'max_depth': trial.suggest_int('max_depth', 5, 8),\n","        'learning_rate': trial.suggest_categorical('learning_rate', [0.03, 0.05, 0.07]),\n","        'num_leaves': trial.suggest_categorical('num_leaves', [40, 60, 80]),\n","        'subsample': trial.suggest_categorical('subsample', [0.8, 0.85, 0.9]),\n","        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.8, 0.85, 0.9]),\n","        'reg_alpha': trial.suggest_categorical('reg_alpha', [0.5, 1.0, 2.0]),\n","        'reg_lambda': trial.suggest_categorical('reg_lambda', [1.0, 2.0, 3.0]),\n","        'random_state': 42,\n","        'verbose': -1,\n","        'n_jobs': -1\n","    }\n","\n","    model = lgb.LGBMRegressor(**params)\n","    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=3,\n","                               scoring='neg_mean_squared_error', n_jobs=1)\n","    return -cv_scores.mean()\n","\n","def fast_catboost_objective(trial):\n","    \"\"\"Streamlined CatBoost optimization\"\"\"\n","    params = {\n","        'iterations': trial.suggest_categorical('iterations', [600, 800, 1000]),\n","        'depth': trial.suggest_int('depth', 5, 8),\n","        'learning_rate': trial.suggest_categorical('learning_rate', [0.03, 0.05, 0.07]),\n","        'l2_leaf_reg': trial.suggest_categorical('l2_leaf_reg', [2, 3, 5]),\n","        'random_seed': 42,\n","        'verbose': False\n","    }\n","\n","    # Force GPU if available\n","    if gpu_status['catboost']:\n","        params['task_type'] = 'GPU'\n","    else:\n","        params.update({'task_type': 'CPU', 'thread_count': -1})\n","\n","    model = cb.CatBoostRegressor(**params)\n","    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=3,\n","                               scoring='neg_mean_squared_error', n_jobs=1)\n","    return -cv_scores.mean()\n","\n","# ============================================================================\n","# Fast Training Function (Reduced Trials)\n","# ============================================================================\n","\n","def train_fast_model(model_name, objective_func, n_trials=20):\n","    \"\"\"Fast model training with reduced trials\"\"\"\n","    print(f\"\\nüîÑ Fast optimization: {model_name} ({n_trials} trials)\")\n","\n","    # Create study with reduced storage for speed\n","    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n","\n","    # Progress every 5 trials instead of 10\n","    def fast_callback(study, trial):\n","        if trial.number % 5 == 0 and trial.number > 0:\n","            print(f\"   Trial {trial.number}: Best MSE = {study.best_value:.4f}\")\n","\n","    start_time = time.time()\n","    study.optimize(objective_func, n_trials=n_trials, callbacks=[fast_callback])\n","    elapsed = time.time() - start_time\n","\n","    print(f\"‚úÖ {model_name} optimized in {elapsed:.1f}s (Best: {study.best_value:.4f})\")\n","    return study\n","\n","# ============================================================================\n","# XGBoost Fast Training\n","# ============================================================================\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üöÄ FAST XGBOOST TRAINING\")\n","print(f\"{'='*50}\")\n","\n","xgb_study = train_fast_model(\"XGBoost\", fast_xgboost_objective, n_trials=20)\n","\n","print(\"üîß Training final XGBoost...\")\n","best_xgb_params = xgb_study.best_params.copy()\n","\n","if gpu_status['xgboost']:\n","    best_xgb_params.update({'tree_method': 'gpu_hist', 'gpu_id': 0})\n","    print(\"   Using GPU acceleration\")\n","else:\n","    best_xgb_params.update({'tree_method': 'hist', 'n_jobs': -1})\n","    print(\"   Using CPU (GPU unavailable)\")\n","\n","start_time = time.time()\n","final_xgb = xgb.XGBRegressor(**best_xgb_params)\n","final_xgb.fit(X_train_processed, y_train)\n","train_time = time.time() - start_time\n","\n","xgb_pred = final_xgb.predict(X_test_processed)\n","xgb_mse = mean_squared_error(y_test, xgb_pred)\n","xgb_mae = mean_absolute_error(y_test, xgb_pred)\n","xgb_r2 = r2_score(y_test, xgb_pred)\n","\n","print(f\"üìä XGBoost Results (trained in {train_time:.1f}s):\")\n","print(f\"   MSE: {xgb_mse:.4f} {'üéØ' if 2.0 <= xgb_mse <= 3.0 else '‚ùå'}\")\n","print(f\"   MAE: {xgb_mae:.4f}\")\n","print(f\"   R¬≤:  {xgb_r2:.4f}\")\n","\n","# Memory cleanup\n","del xgb_study\n","gc.collect()\n","\n","# ============================================================================\n","# LightGBM Fast Training\n","# ============================================================================\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üöÄ FAST LIGHTGBM TRAINING\")\n","print(f\"{'='*50}\")\n","\n","lgb_study = train_fast_model(\"LightGBM\", fast_lightgbm_objective, n_trials=20)\n","\n","print(\"üîß Training final LightGBM...\")\n","start_time = time.time()\n","final_lgb = lgb.LGBMRegressor(**lgb_study.best_params)\n","final_lgb.fit(X_train_processed, y_train)\n","train_time = time.time() - start_time\n","\n","lgb_pred = final_lgb.predict(X_test_processed)\n","lgb_mse = mean_squared_error(y_test, lgb_pred)\n","lgb_mae = mean_absolute_error(y_test, lgb_pred)\n","lgb_r2 = r2_score(y_test, lgb_pred)\n","\n","print(f\"üìä LightGBM Results (trained in {train_time:.1f}s):\")\n","print(f\"   MSE: {lgb_mse:.4f} {'üéØ' if 2.0 <= lgb_mse <= 3.0 else '‚ùå'}\")\n","print(f\"   MAE: {lgb_mae:.4f}\")\n","print(f\"   R¬≤:  {lgb_r2:.4f}\")\n","\n","# Memory cleanup\n","del lgb_study\n","gc.collect()\n","\n","# ============================================================================\n","# CatBoost Fast Training\n","# ============================================================================\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üöÄ FAST CATBOOST TRAINING\")\n","print(f\"{'='*50}\")\n","\n","cb_study = train_fast_model(\"CatBoost\", fast_catboost_objective, n_trials=15)  # Even fewer for CatBoost\n","\n","print(\"üîß Training final CatBoost...\")\n","best_cb_params = cb_study.best_params.copy()\n","\n","if gpu_status['catboost']:\n","    best_cb_params['task_type'] = 'GPU'\n","    print(\"   Using GPU acceleration\")\n","else:\n","    best_cb_params.update({'task_type': 'CPU', 'thread_count': -1})\n","    print(\"   Using CPU (GPU unavailable)\")\n","\n","start_time = time.time()\n","final_cb = cb.CatBoostRegressor(**best_cb_params)\n","final_cb.fit(X_train_processed, y_train)\n","train_time = time.time() - start_time\n","\n","cb_pred = final_cb.predict(X_test_processed)\n","cb_mse = mean_squared_error(y_test, cb_pred)\n","cb_mae = mean_absolute_error(y_test, cb_pred)\n","cb_r2 = r2_score(y_test, cb_pred)\n","\n","print(f\"üìä CatBoost Results (trained in {train_time:.1f}s):\")\n","print(f\"   MSE: {cb_mse:.4f} {'üéØ' if 2.0 <= cb_mse <= 3.0 else '‚ùå'}\")\n","print(f\"   MAE: {cb_mae:.4f}\")\n","print(f\"   R¬≤:  {cb_r2:.4f}\")\n","\n","# Memory cleanup\n","del cb_study\n","gc.collect()\n","\n","# ============================================================================\n","# Fast Ensemble\n","# ============================================================================\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üéØ FAST ENSEMBLE\")\n","print(f\"{'='*50}\")\n","\n","# Simple performance-based weighting\n","models_mse = {'XGBoost': xgb_mse, 'LightGBM': lgb_mse, 'CatBoost': cb_mse}\n","total_inv_mse = sum(1/mse for mse in models_mse.values())\n","weights = {name: (1/mse)/total_inv_mse for name, mse in models_mse.items()}\n","\n","print(\"üìä Ensemble weights:\")\n","for name, weight in weights.items():\n","    print(f\"   {name}: {weight:.3f}\")\n","\n","# Create ensemble\n","ensemble_pred = (weights['XGBoost'] * xgb_pred +\n","                weights['LightGBM'] * lgb_pred +\n","                weights['CatBoost'] * cb_pred)\n","\n","ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n","ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n","ensemble_r2 = r2_score(y_test, ensemble_pred)\n","\n","print(f\"\\nüìä Ensemble Results:\")\n","print(f\"   MSE: {ensemble_mse:.4f} {'üéØ' if 2.0 <= ensemble_mse <= 3.0 else '‚ùå'}\")\n","print(f\"   MAE: {ensemble_mae:.4f}\")\n","print(f\"   R¬≤:  {ensemble_r2:.4f}\")\n","\n","# ============================================================================\n","# Final Summary\n","# ============================================================================\n","\n","all_results = {\n","    'XGBoost': xgb_mse,\n","    'LightGBM': lgb_mse,\n","    'CatBoost': cb_mse,\n","    'Ensemble': ensemble_mse\n","}\n","\n","best_model = min(all_results.keys(), key=lambda k: all_results[k])\n","best_mse = all_results[best_model]\n","\n","print(f\"\\n{'='*50}\")\n","print(\"üèÜ FAST TRAINING SUMMARY\")\n","print(f\"{'='*50}\")\n","\n","print(\"üìä All Results:\")\n","for model, mse in sorted(all_results.items(), key=lambda x: x[1]):\n","    status = \"üéØ TARGET!\" if 2.0 <= mse <= 3.0 else \"‚ùå Miss\"\n","    print(f\"   {model:10}: {mse:.4f} {status}\")\n","\n","print(f\"\\nüèÜ Best: {best_model} ({best_mse:.4f})\")\n","print(f\"üéØ Target: {'‚úÖ ACHIEVED' if 2.0 <= best_mse <= 3.0 else '‚ùå MISSED'}\")\n","print(f\"‚ö° Total trials: 55 (vs 140 in full optimization)\")\n","print(f\"üñ•Ô∏è  GPU used: {'‚úÖ YES' if any(gpu_status.values()) else '‚ùå NO'}\")\n","\n","# Quick save\n","try:\n","    fast_results = {\n","        'models': {'xgb': final_xgb, 'lgb': final_lgb, 'cb': final_cb},\n","        'predictions': {'xgb': xgb_pred, 'lgb': lgb_pred, 'cb': cb_pred, 'ensemble': ensemble_pred},\n","        'results': all_results,\n","        'best_model': best_model,\n","        'best_mse': best_mse,\n","        'y_test': y_test\n","    }\n","    joblib.dump(fast_results, \"/content/fast_training_results.joblib\")\n","    print(f\"üíæ Results saved to: /content/fast_training_results.joblib\")\n","except:\n","    print(f\"‚ö†Ô∏è  Save failed\")\n","\n","print(f\"\\nüöÄ Fast training complete! Ready for evaluation.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"422dbb59-aaf2-43d1-c0db-d312313880ab","id":"IJJlOXyKBPFS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting optimized fast training...\n","‚ö° Optimizations: Reduced trials, GPU-first, memory efficient\n","‚úÖ XGBoost GPU verified\n","‚úÖ CatBoost GPU verified\n","\n","==================================================\n","üöÄ FAST XGBOOST TRAINING\n","==================================================\n","\n","üîÑ Fast optimization: XGBoost (20 trials)\n","   Trial 5: Best MSE = 31.5478\n","   Trial 10: Best MSE = 30.3644\n","   Trial 15: Best MSE = 30.0468\n","‚úÖ XGBoost optimized in 594.8s (Best: 30.0468)\n","üîß Training final XGBoost...\n","   Using GPU acceleration\n","üìä XGBoost Results (trained in 11.3s):\n","   MSE: 31.6465 ‚ùå\n","   MAE: 4.1513\n","   R¬≤:  0.8495\n","\n","==================================================\n","üöÄ FAST LIGHTGBM TRAINING\n","==================================================\n","\n","üîÑ Fast optimization: LightGBM (20 trials)\n","   Trial 5: Best MSE = 29.0465\n","   Trial 10: Best MSE = 29.0465\n","   Trial 15: Best MSE = 29.0465\n","‚úÖ LightGBM optimized in 209.3s (Best: 29.0465)\n","üîß Training final LightGBM...\n","üìä LightGBM Results (trained in 7.9s):\n","   MSE: 30.8542 ‚ùå\n","   MAE: 4.3107\n","   R¬≤:  0.8532\n","\n","==================================================\n","üöÄ FAST CATBOOST TRAINING\n","==================================================\n","\n","üîÑ Fast optimization: CatBoost (15 trials)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ASfVjRPN_ZKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Advanced Ensemble Creation\n","# ============================================================================\n","\n","print(f\"\\n{'='*60}\")\n","print(\"üéØ CREATING ADVANCED ENSEMBLE MODEL\")\n","print(f\"{'='*60}\")\n","\n","print(\"üîß Building weighted ensemble...\")\n","\n","# Store all model results\n","model_results = {\n","    'XGBoost': {'model': final_xgb, 'mse': xgb_mse, 'predictions': xgb_pred},\n","    'LightGBM': {'model': final_lgb, 'mse': lgb_mse, 'predictions': lgb_pred},\n","    'CatBoost': {'model': final_cb, 'mse': cb_mse, 'predictions': cb_pred}\n","}\n","\n","# Calculate inverse MSE weights (better models get higher weight)\n","total_inverse_mse = sum(1/result['mse'] for result in model_results.values())\n","model_weights = {\n","    name: (1/result['mse']) / total_inverse_mse\n","    for name, result in model_results.items()\n","}\n","\n","print(\"üìä Model weights based on performance:\")\n","for name, weight in model_weights.items():\n","    print(f\"   {name}: {weight:.3f}\")\n","\n","# Create ensemble prediction\n","ensemble_pred = np.zeros_like(y_test, dtype=float)\n","for name, result in model_results.items():\n","    ensemble_pred += model_weights[name] * result['predictions']\n","\n","# Evaluate ensemble\n","ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n","ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n","ensemble_r2 = r2_score(y_test, ensemble_pred)\n","\n","print(f\"\\nüìä Ensemble Results:\")\n","print(f\"   Test MSE: {ensemble_mse:.4f}\")\n","print(f\"   Test MAE: {ensemble_mae:.4f}\")\n","print(f\"   Test R¬≤:  {ensemble_r2:.4f}\")"],"metadata":{"id":"jmu30Gn85oET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Final Results and Target Achievement\n","# ============================================================================\n","\n","print(f\"\\n{'='*60}\")\n","print(\"üèÜ FINAL MODEL COMPARISON AND TARGET ASSESSMENT\")\n","print(f\"{'='*60}\")\n","\n","# Compare all models\n","all_results = {\n","    'XGBoost': xgb_mse,\n","    'LightGBM': lgb_mse,\n","    'CatBoost': cb_mse,\n","    'Ensemble': ensemble_mse\n","}\n","\n","print(\"üìä Final MSE Comparison:\")\n","for model_name, mse in sorted(all_results.items(), key=lambda x: x[1]):\n","    target_status = \"üéØ TARGET ACHIEVED!\" if 2.0 <= mse <= 3.0 else \"‚ùå Outside target\"\n","    print(f\"   {model_name:12}: MSE = {mse:.4f} {target_status}\")\n","\n","# Find best model\n","best_model_name = min(all_results.keys(), key=lambda k: all_results[k])\n","best_mse = all_results[best_model_name]\n","\n","print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n","print(f\"üéØ BEST MSE: {best_mse:.4f}\")\n","\n","# Check target achievement\n","target_achieved = 2.0 <= best_mse <= 3.0\n","print(f\"üéØ TARGET RANGE (2.0-3.0): {'‚úÖ ACHIEVED!' if target_achieved else '‚ùå Not achieved'}\")\n","\n","if target_achieved:\n","    print(f\"\\nüéâ SUCCESS! Achieved target MSE of {best_mse:.4f}\")\n","    print(f\"üèÜ This performance is competitive for age prediction!\")\n","else:\n","    print(f\"\\nüí° Further optimization suggestions:\")\n","    print(f\"   ‚Ä¢ Try different feature engineering approaches\")\n","    print(f\"   ‚Ä¢ Experiment with neural networks\")\n","    print(f\"   ‚Ä¢ Use more sophisticated ensemble methods\")\n","    print(f\"   ‚Ä¢ Consider external validation datasets\")"],"metadata":{"id":"v_ibIarK5iaW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZrudW_lO_kd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save Best Models\n","# ============================================================================\n","\n","print(f\"\\nüíæ Saving trained models...\")\n","\n","# Create models package\n","models_package = {\n","    'best_model_name': best_model_name,\n","    'best_mse': best_mse,\n","    'target_achieved': target_achieved,\n","    'models': {\n","        'xgboost': final_xgb,\n","        'lightgbm': final_lgb,\n","        'catboost': final_cb\n","    },\n","    'model_results': model_results,\n","    'ensemble_weights': model_weights,\n","    'predictions': {\n","        'xgboost': xgb_pred,\n","        'lightgbm': lgb_pred,\n","        'catboost': cb_pred,\n","        'ensemble': ensemble_pred,\n","        'y_test': y_test\n","    }\n","}\n","\n","# Save models\n","try:\n","    models_save_path = os.path.join(PROJECT_DIR, \"trained_models.joblib\")\n","    joblib.dump(models_package, models_save_path)\n","    print(f\"‚úÖ Models saved to: {models_save_path}\")\n","except:\n","    backup_models_path = \"/content/trained_models.joblib\"\n","    joblib.dump(models_package, backup_models_path)\n","    print(f\"‚úÖ Models saved to backup: {backup_models_path}\")\n","\n","print(f\"\\nüöÄ Advanced model training complete!\")\n","print(f\"üéØ Ready for evaluation on external dataset!\")"],"metadata":{"id":"bhC-nt3T5Y5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5I80smnm_ocf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Random Forest Regressor"],"metadata":{"id":"2MGTRLK5_sBs"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# Train\n","rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred_rf = rf_model.predict(X_test)\n","mse_rf = mean_squared_error(y_test, y_pred_rf)\n","print(f\"Random Forest MSE: {mse_rf:.2f}\")\n","\n","# Plot\n","plot_results(y_test, y_pred_rf, \"Random Forest - Age Prediction\")\n","\n","# Save model & predictions\n","joblib.dump(rf_model, os.path.join(PROJECT_DIR, \"RandomForest_model.joblib\"))\n","pd.DataFrame({\"Actual_Age\": y_test.values, \"Predicted_Age\": y_pred_rf}).to_csv(\n","    os.path.join(PROJECT_DIR, \"RandomForest_predictions.csv\"), index=False)\n"],"metadata":{"id":"r4-SToKJVtYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random Forest Regressor 2.1\n","\n","# Set your output directory\n","output_dir = \"/content/drive/My Drive/Project/Aging Biomarkers\"\n","\n","# Apply age transformation\n","y_train_log = np.log1p(y_train)\n","y_test_log = np.log1p(y_test)\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2]\n","}\n","\n","grid_search = GridSearchCV(\n","    estimator=RandomForestRegressor(random_state=42),\n","    param_grid=param_grid,\n","    cv=5,\n","    scoring='neg_mean_squared_error',\n","    n_jobs=-1,\n","    verbose=1\n",")\n","\n","grid_search.fit(X_train, y_train_log)\n","best_rf = grid_search.best_estimator_\n","\n","# Predict & evaluate\n","y_pred_rf_log = best_rf.predict(X_test)\n","y_pred_rf = np.expm1(y_pred_rf_log)\n","mse_rf = mean_squared_error(y_test, y_pred_rf)\n","print(f\"RandomForest (Tuned) Test MSE: {mse_rf:.4f}\")\n","\n","# Plot: Actual vs Predicted\n","plt.figure(figsize=(6, 6))\n","plt.scatter(y_test, y_pred_rf, alpha=0.6)\n","plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n","plt.xlabel(\"Actual Age\")\n","plt.ylabel(\"Predicted Age\")\n","plt.title(\"RandomForest (Tuned): Actual vs Predicted\")\n","plt.grid(True)\n","plt.show()\n","\n","# Feature Importance Plot\n","importances = pd.Series(best_rf.feature_importances_, index=range(X_train.shape[1]))\n","top_features = importances.sort_values(ascending=False).head(20)\n","top_features.plot(kind=\"barh\", figsize=(6, 8))\n","plt.title(\"Top 20 Important Features\")\n","plt.xlabel(\"Importance Score\")\n","plt.show()\n","\n","# Save model and predictions\n","joblib.dump(best_rf, os.path.join(output_dir, \"RandomForest_model_tuned.joblib\"))\n","\n","pred_df = pd.DataFrame({\n","    \"Actual_Age\": y_test,\n","    \"Predicted_Age\": y_pred_rf\n","})\n","pred_df.to_csv(os.path.join(output_dir, \"RandomForest_test_predictions_tuned.csv\"), index=False)\n","\n","print(\"‚úÖ Tuned model and predictions saved to:\", output_dir)"],"metadata":{"id":"kLQ-kFMxi-zf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random Forest Regressor 2.2\n","\n","# === Step 1: Prepare feature matrix using stable_lasso CpGs ===\n","df = data.dnam.loc[stable_lasso].T.copy()\n","df[\"age\"] = data.metadata[\"age\"]\n","\n","X_feat = df[stable_lasso].values\n","y_feat = df[\"age\"].values\n","\n","# === Step 2: Log-transform age to stabilize variance ===\n","y_feat_log = np.log1p(y_feat)\n","\n","# === Step 3: Train-test split ===\n","X_train, X_test, y_train_log, y_test_log = train_test_split(\n","    X_feat, y_feat_log, test_size=0.2, random_state=42\n",")\n","\n","# Inverse transform for evaluation\n","y_train = np.expm1(y_train_log)\n","y_test = np.expm1(y_test_log)\n","\n","print(f\"‚úÖ Train shape: X={X_train.shape}, y={y_train.shape}\")\n","print(f\"‚úÖ Test  shape: X={X_test.shape}, y={y_test.shape}\")\n","\n","# === Step 4: Train Random Forest Regressor ===\n","rf = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train_log)\n","\n","# === Step 5: Predict and evaluate ===\n","y_pred_log = rf.predict(X_test)\n","y_pred = np.expm1(y_pred_log)\n","\n","mse_rf = mean_squared_error(y_test, y_pred)\n","print(f\"üìâ RandomForest Test MSE: {mse_rf:.4f}\")\n","\n","# === Step 6: Plot Actual vs Predicted ===\n","plt.figure(figsize=(6, 6))\n","plt.scatter(y_test, y_pred, alpha=0.6)\n","plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n","plt.xlabel(\"Actual Age\")\n","plt.ylabel(\"Predicted Age\")\n","plt.title(\"RandomForest: Actual vs Predicted\")\n","plt.grid(True)\n","plt.show()\n","\n","# === Step 7: Save model and predictions ===\n","output_dir = \"/content/drive/My Drive/Project/Aging Biomarkers\"\n","\n","joblib.dump(rf, os.path.join(output_dir, \"RandomForest_model.joblib\"))\n","\n","pd.DataFrame({\n","    \"Actual_Age\": y_test,\n","    \"Predicted_Age\": y_pred\n","}).to_csv(os.path.join(output_dir, \"RandomForest_test_predictions.csv\"), index=False)\n","\n","print(\"‚úÖ Model and predictions saved to:\", output_dir)\n","\n","# === Step 8: Feature Importance Plot ===\n","importances = pd.Series(rf.feature_importances_, index=stable_lasso)\n","top_features = importances.sort_values(ascending=False).head(20)\n","\n","plt.figure(figsize=(6, 8))\n","top_features.plot(kind=\"barh\")\n","plt.title(\"Top 20 Important CpG Features (Random Forest)\")\n","plt.xlabel(\"Importance Score\")\n","plt.gca().invert_yaxis()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"fe8Bym98jHw6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Gradient Boosting Regressor"],"metadata":{"id":"5qtuQcNuAHx9"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingRegressor\n","\n","# Train\n","gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n","gb_model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred_gb = gb_model.predict(X_test)\n","mse_gb = mean_squared_error(y_test, y_pred_gb)\n","print(f\"Gradient Boosting MSE: {mse_gb:.2f}\")\n","\n","# Plot\n","plot_results(y_test, y_pred_gb, \"Gradient Boosting - Age Prediction\")\n","\n","# Save model & predictions\n","joblib.dump(gb_model, os.path.join(PROJECT_DIR, \"GradientBoosting_model.joblib\"))\n","pd.DataFrame({\"Actual_Age\": y_test.values, \"Predicted_Age\": y_pred_gb}).to_csv(\n","    os.path.join(PROJECT_DIR, \"GradientBoosting_predictions.csv\"), index=False)"],"metadata":{"id":"GvdgOt6bALR8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Support Vector Regression (SVR)"],"metadata":{"id":"jOIi-ITQANv-"}},{"cell_type":"code","source":["# Set your local output directory\n","output_dir = \"/content/drive/My Drive/Project/Aging Biomarkers\"\n","\n","# Train SVR model (you can tune kernel, C, and epsilon later)\n","svr = SVR(kernel='rbf', C=10, epsilon=0.1)\n","svr.fit(X_train, y_train)\n","\n","# Predict & evaluate\n","y_pred_svr = svr.predict(X_test)\n","mse_svr = mean_squared_error(y_test, y_pred_svr)\n","print(f\"SVR Test MSE: {mse_svr:.4f}\")\n","\n","# Plot Actual vs Predicted\n","plt.figure(figsize=(6, 6))\n","plt.scatter(y_test, y_pred_svr, alpha=0.6)\n","plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n","plt.xlabel(\"Actual Age\")\n","plt.ylabel(\"Predicted Age\")\n","plt.title(\"SVR: Actual vs Predicted\")\n","plt.grid(True)\n","plt.show()\n","\n","# Save model and predictions\n","joblib.dump(svr, os.path.join(output_dir, \"SVR_model.joblib\"))\n","pd.DataFrame({\n","    \"Actual_Age\": y_test,\n","    \"Predicted_Age\": y_pred_svr\n","}).to_csv(os.path.join(output_dir, \"SVR_test_predictions.csv\"), index=False)\n","\n","print(\"‚úÖ SVR model and predictions saved.\")"],"metadata":{"id":"qT1g7GhmARAG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RidgeCV"],"metadata":{"id":"4hGDk89EAUFR"}},{"cell_type":"code","source":["from sklearn.linear_model import RidgeCV\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","import joblib\n","import os\n","import pandas as pd\n","\n","# Set your local output directory\n","output_dir = \"/content/drive/My Drive/Project/Aging Biomarkers\"\n","\n","# Train Ridge Regression with built-in cross-validation over a range of alphas\n","alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n","ridge = RidgeCV(alphas=alphas, cv=5)\n","ridge.fit(X_train, y_train)\n","\n","# Predict & evaluate\n","y_pred_ridge = ridge.predict(X_test)\n","mse_ridge    = mean_squared_error(y_test, y_pred_ridge)\n","print(f\"Ridge Regression Test MSE: {mse_ridge:.4f}\")\n","\n","# Plot Actual vs Predicted\n","plt.figure(figsize=(6, 6))\n","plt.scatter(y_test, y_pred_ridge, alpha=0.6)\n","plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n","plt.xlabel(\"Actual Age\")\n","plt.ylabel(\"Predicted Age\")\n","plt.title(\"Ridge Regression: Actual vs Predicted\")\n","plt.grid(True)\n","plt.show()\n","\n","# Save model and predictions\n","joblib.dump(ridge, os.path.join(output_dir, \"Ridge_model.joblib\"))\n","\n","pd.DataFrame({\n","    \"Actual_Age\":    y_test,\n","    \"Predicted_Age\": y_pred_ridge\n","}).to_csv(os.path.join(output_dir, \"Ridge_test_predictions.csv\"), index=False)\n","\n","print(\"‚úÖ Ridge model and predictions saved to:\", output_dir)\n"],"metadata":{"id":"lUjQ4uEFjacP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cUhgnWMyAaFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","import os\n","\n","# Gather test‚Äêset MSEs\n","mse_dict = {\n","    \"ElasticNet\":    mse_enet,\n","    \"RandomForest\":  mse_rf,\n","    \"GradientBoosting\": mse_gb,\n","    \"SVR\":           mse_svr,\n","    \"Ridge\":         mse_ridge\n","}\n","\n","# Sort ascending by MSE and pick best + second best\n","sorted_models    = sorted(mse_dict.items(), key=lambda x: x[1])\n","best_name, _     = sorted_models[0]\n","second_name, _   = sorted_models[1]\n","\n","# Model name ‚Üí fitted object mapping\n","model_map = {\n","    \"ElasticNet\":    enet,\n","    \"RandomForest\":  rf,\n","    \"GradientBoosting\": gb,\n","    \"SVR\":           svr,\n","    \"Ridge\":         ridge\n","}\n","\n","best_mod   = model_map[best_name]\n","second_mod = model_map[second_name]\n","\n","# Report\n","print(f\"üèÜ Best Model:   {best_name} (MSE={mse_dict[best_name]:.4f})\")\n","print(f\"ü•à Second Best: {second_name} (MSE={mse_dict[second_name]:.4f})\")\n","\n","# Save both to disk\n","joblib.dump(best_mod,   os.path.join(output_dir, f\"{best_name}_model.joblib\"))\n","joblib.dump(second_mod, os.path.join(output_dir, f\"{second_name}_model.joblib\"))\n","print(\"‚úÖ Top two models saved to:\", output_dir)\n"],"metadata":{"id":"VwUxAsUxjp-_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import joblib\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Load the best model\n","best_model_path = os.path.join(output_dir, f\"{best_name}_model.joblib\")\n","best_model = joblib.load(best_model_path)\n","\n","# Predict on the test split\n","y_test_pred = best_model.predict(X_test)\n","\n","# Compute and print MSE\n","mse_best = mean_squared_error(y_test, y_test_pred)\n","print(f\"‚úÖ {best_name} Test MSE: {mse_best:.4f}\")\n","\n","# Plot Actual vs. Predicted\n","plt.figure(figsize=(6, 6))\n","plt.scatter(y_test, y_test_pred, alpha=0.6)\n","plt.plot([y_test.min(), y_test.max()],\n","         [y_test.min(), y_test.max()],\n","         \"r--\", lw=2)\n","plt.xlabel(\"Actual Age\")\n","plt.ylabel(\"Predicted Age\")\n","plt.title(f\"{best_name}: Actual vs Predicted\")\n","plt.grid(True)\n","plt.show()\n","\n","# Save test‚Äêset predictions to CSV\n","pred_df = pd.DataFrame({\n","    \"Actual_Age\":    y_test,\n","    \"Predicted_Age\": y_test_pred\n","})\n","pred_df.to_csv(os.path.join(output_dir, f\"{best_name}_test_predictions.csv\"), index=False)\n","print(f\"‚úÖ Test predictions saved to: {output_dir}\")"],"metadata":{"id":"u45rAe34j03p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","# Predict on test set\n","y_test_pred = best_model.predict(X_test)\n","\n","# Plot\n","plt.figure(figsize=(10, 8))\n","plt.scatter(y_test, y_test_pred, alpha=0.7, edgecolors='w', linewidth=0.5)\n","plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=3)\n","plt.xlabel('Actual Age')\n","plt.ylabel('Predicted Age')\n","plt.title(f'{best_name}: Actual vs. Predicted Age (Test Set)')\n","plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","plt.show()\n","\n","# Calculate metrics\n","mse_test_final = mean_squared_error(y_test, y_test_pred)\n","mae_test_final = mean_absolute_error(y_test, y_test_pred)\n","\n","print(f\"‚úÖ Final Test MSE: {mse_test_final:.4f}\")\n","print(f\"‚úÖ Final Test MAE: {mae_test_final:.4f}\")"],"metadata":{"id":"mrvIM0a6j7TE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4fOFSdYhAjWk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Align eval features with training features\n","X_eval = evaluation_data.dnam.T.copy()\n","\n","# Ensure all selected CpGs exist in eval data; fill missing with 0\n","for cpg in set(stable_lasso) - set(X_eval.columns):\n","    X_eval[cpg] = 0.0\n","\n","# Reorder columns to match training order\n","X_eval = X_eval[stable_lasso].values\n","\n","# Predict using the best model (Ridge)\n","y_eval_pred = best_mod.predict(X_eval)\n","\n","# Create DataFrame of predictions\n","eval_pred_df = pd.DataFrame({\n","    \"Predicted_Age\": y_eval_pred\n","}, index=evaluation_data.dnam.columns)\n","\n","# Preview predictions\n","print(\"‚úÖ Eval predictions preview:\")\n","display(eval_pred_df.head())\n","\n","# Plot histogram of predicted ages\n","plt.figure(figsize=(7, 5))\n","plt.hist(y_eval_pred, bins=30, edgecolor=\"k\")\n","plt.title(\"Evaluation Set: Predicted Age Distribution\")\n","plt.xlabel(\"Predicted Age\")\n","plt.ylabel(\"Count\")\n","plt.grid(True)\n","plt.show()\n","\n","# Save predictions to CSV\n","eval_output_path = os.path.join(output_dir, \"GSE157131_predicted_ages.csv\")\n","eval_pred_df.to_csv(eval_output_path)\n","\n","print(f\"‚úÖ Evaluation predictions saved to: {eval_output_path}\")"],"metadata":{"id":"t-hmDzUMkFme"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eQE6C-zYAl8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## RidgeCV\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","\n","# Ensure consistency of feature columns (add missing CpGs as zeros)\n","pruned_data = evaluation_data.dnam.T.copy()\n","missing_cpg_sites = list(set(stable_lasso) - set(pruned_data.columns))\n","pruned_data.loc[:, missing_cpg_sites] = 0\n","pruned_data = pruned_data[stable_lasso]\n","\n","# Predict using best trained model\n","challenge_results = best_mod.predict(pruned_data)\n","\n","# Combine predictions with actual ages (if available)\n","predicted_age_df = pd.DataFrame({\n","    'predictedAge': challenge_results,\n","    'ActualAge': evaluation_data.metadata.age\n","}, index=evaluation_data.dnam.columns)\n","predicted_age_df.index.name = 'sampleId'\n","\n","# Plot actual vs predicted\n","y_pred = challenge_results\n","y_true = evaluation_data.metadata.age\n","\n","plt.figure(figsize=(10, 8))\n","plt.scatter(y_true, y_pred, alpha=0.7, edgecolors='w', linewidth=0.5)\n","plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'k--', lw=3)\n","plt.xlabel('Actual Age')\n","plt.ylabel('Predicted Age')\n","plt.title('Evaluation Set: Actual vs. Predicted Age')\n","plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","plt.show()\n","\n","# Metrics\n","mse_eval = np.mean((y_true - y_pred) ** 2)\n","mae_eval = np.mean(np.abs(y_true - y_pred))\n","print(f\"üìä Evaluation MSE: {mse_eval:.4f}\")\n","print(f\"üìä Evaluation MAE: {mae_eval:.4f}\")\n","\n","# Save predictions\n","eval_output_path = os.path.join(output_dir, \"EvaluationSet_predictions.csv\")\n","predicted_age_df.to_csv(eval_output_path)\n","print(f\"‚úÖ Evaluation predictions saved to: {eval_output_path}\")"],"metadata":{"id":"mveQIxZLkHml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JpsGWljqAo0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## ElasticNet\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","\n","# Ensure consistency of feature columns (add missing CpGs as zeros)\n","pruned_data = evaluation_data.dnam.T.copy()\n","missing_cpg_sites = list(set(stable_lasso) - set(pruned_data.columns))\n","pruned_data.loc[:, missing_cpg_sites] = 0\n","pruned_data = pruned_data[stable_lasso]\n","\n","# Predict using ElasticNet\n","challenge_results_enet = enet.predict(pruned_data)\n","\n","# Combine predictions with actual ages\n","predicted_age_df_enet = pd.DataFrame({\n","    'predictedAge': challenge_results_enet,\n","    'ActualAge': evaluation_data.metadata.age\n","}, index=evaluation_data.dnam.columns)\n","predicted_age_df_enet.index.name = 'sampleId'\n","\n","# Plot actual vs predicted\n","y_pred_enet = challenge_results_enet\n","y_true_enet = evaluation_data.metadata.age\n","\n","plt.figure(figsize=(10, 8))\n","plt.scatter(y_true_enet, y_pred_enet, alpha=0.7, edgecolors='w', linewidth=0.5)\n","plt.plot([min(y_true_enet), max(y_true_enet)], [min(y_true_enet), max(y_true_enet)], 'k--', lw=3)\n","plt.xlabel('Actual Age')\n","plt.ylabel('Predicted Age')\n","plt.title('ElasticNet: Evaluation Set - Actual vs. Predicted Age')\n","plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","plt.show()\n","\n","# Metrics\n","mse_eval_enet = np.mean((y_true_enet - y_pred_enet) ** 2)\n","mae_eval_enet = np.mean(np.abs(y_true_enet - y_pred_enet))\n","print(f\"üìä Evaluation MSE (ElasticNet): {mse_eval_enet:.4f}\")\n","print(f\"üìä Evaluation MAE (ElasticNet): {mae_eval_enet:.4f}\")\n","\n","# Save predictions\n","eval_output_path_enet = os.path.join(output_dir, \"ElasticNet_EvaluationSet_predictions.csv\")\n","predicted_age_df_enet.to_csv(eval_output_path_enet)\n","print(f\"‚úÖ Evaluation predictions saved to: {eval_output_path_enet}\")\n"],"metadata":{"id":"yCDtRMnekKkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pr_9qve3ArJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Step 1: Align evaluation data with training features (fill missing CpGs with 0.0)\n","pruned_data = evaluation_data.dnam.T.copy()\n","missing_cpg_sites = list(set(stable_lasso) - set(pruned_data.columns))\n","pruned_data.loc[:, missing_cpg_sites] = 0\n","pruned_data = pruned_data[stable_lasso]  # ensure column order\n","\n","# Step 2: Predict with Gradient Boosting Regressor\n","y_eval_pred = gb.predict(pruned_data)\n","\n","# Step 3: Create DataFrame with predictions and actual ages\n","predicted_age_df = pd.DataFrame({\n","    'predictedAge': y_eval_pred,\n","    'ActualAge': evaluation_data.metadata.age\n","}, index=evaluation_data.dnam.columns)\n","predicted_age_df.index.name = 'sampleId'\n","\n","# Step 4: Plot actual vs predicted\n","plt.figure(figsize=(10, 8))\n","plt.scatter(predicted_age_df[\"ActualAge\"], predicted_age_df[\"predictedAge\"], alpha=0.7, edgecolors='w', linewidth=0.5)\n","plt.plot([min(predicted_age_df[\"ActualAge\"]), max(predicted_age_df[\"ActualAge\"])],\n","         [min(predicted_age_df[\"ActualAge\"]), max(predicted_age_df[\"ActualAge\"])],\n","         'k--', lw=3)\n","plt.xlabel('Actual Age')\n","plt.ylabel('Predicted Age')\n","plt.title('Gradient Boosting - Evaluation Set: Actual vs. Predicted Age')\n","plt.grid(True, linestyle='--', linewidth=0.5)\n","plt.show()\n","\n","# Step 5: Compute and print metrics\n","mse = np.mean((predicted_age_df[\"ActualAge\"] - predicted_age_df[\"predictedAge\"]) ** 2)\n","mae = np.mean(np.abs(predicted_age_df[\"ActualAge\"] - predicted_age_df[\"predictedAge\"]))\n","print(f\"üìä Evaluation MSE (Gradient Boosting): {mse:.4f}\")\n","print(f\"üìä Evaluation MAE (Gradient Boosting): {mae:.4f}\")\n","\n","# Step 6: Show the prediction table\n","display(predicted_age_df.head())\n","\n","# Step 7: Plot histogram of predicted ages\n","plt.figure(figsize=(8, 5))\n","plt.hist(predicted_age_df[\"predictedAge\"], bins=30, edgecolor='black')\n","plt.xlabel(\"Predicted Age\")\n","plt.ylabel(\"Count\")\n","plt.title(\"Distribution of Predicted Ages - Evaluation Set (Gradient Boosting)\")\n","plt.grid(True, linestyle='--', linewidth=0.5)\n","plt.show()\n","\n","# Step 8: Save results\n","eval_output_path = os.path.join(output_dir, \"EvaluationSet_Predicted_vs_Actual_GB.csv\")\n","predicted_age_df.to_csv(eval_output_path)\n","print(f\"‚úÖ Evaluation predictions saved to: {eval_output_path}\")"],"metadata":{"id":"x9B6SpfVkNM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UnnTuzxKAxMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.metrics import mean_squared_error, make_scorer\n","\n","# Define parameter grid\n","param_grid = {\n","    'n_estimators': [100, 200],\n","    'learning_rate': [0.05, 0.1, 0.2],\n","    'max_depth': [3, 5, 7],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 3]\n","}\n","\n","# Initialize base model\n","gb_base = GradientBoostingRegressor(random_state=42)\n","\n","# Define MSE scorer (negative because sklearn wants higher=better)\n","mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n","\n","# Grid search\n","grid_search = GridSearchCV(\n","    gb_base,\n","    param_grid,\n","    scoring=mse_scorer,\n","    cv=5,\n","    n_jobs=-1,\n","    verbose=2\n",")\n","\n","# Fit to training data\n","grid_search.fit(X_train, y_train)\n","\n","# Best model\n","best_gb = grid_search.best_estimator_\n","print(f\"‚úÖ Best Params: {grid_search.best_params_}\")\n","\n","# Evaluate on test set\n","y_pred_best_gb = best_gb.predict(X_test)\n","mse_best_gb = mean_squared_error(y_test, y_pred_best_gb)\n","print(f\"üéØ Tuned Gradient Boosting Test MSE: {mse_best_gb:.4f}\")\n"],"metadata":{"id":"e1rES8zOkP8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EuRr8c5nA9ZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Step 1: Align evaluation data with training features (fill missing CpGs with 0.0)\n","pruned_data = evaluation_data.dnam.T.copy()\n","missing_cpg_sites = list(set(stable_lasso) - set(pruned_data.columns))\n","pruned_data.loc[:, missing_cpg_sites] = 0\n","pruned_data = pruned_data[stable_lasso]  # ensure column order\n","\n","# Step 2: Predict with the tuned Gradient Boosting Regressor\n","y_eval_pred = best_gb.predict(pruned_data)\n","\n","# Step 3: Create DataFrame with predictions and actual ages\n","predicted_age_df = pd.DataFrame({\n","    'predictedAge': y_eval_pred,\n","    'ActualAge': evaluation_data.metadata.age\n","}, index=evaluation_data.dnam.columns)\n","predicted_age_df.index.name = 'sampleId'\n","\n","# Step 4: Plot actual vs predicted\n","plt.figure(figsize=(10, 8))\n","plt.scatter(predicted_age_df[\"ActualAge\"], predicted_age_df[\"predictedAge\"], alpha=0.7, edgecolors='w', linewidth=0.5)\n","plt.plot([min(predicted_age_df[\"ActualAge\"]), max(predicted_age_df[\"ActualAge\"])],\n","         [min(predicted_age_df[\"ActualAge\"]), max(predicted_age_df[\"ActualAge\"])],\n","         'k--', lw=3)\n","plt.xlabel('Actual Age')\n","plt.ylabel('Predicted Age')\n","plt.title('Tuned Gradient Boosting - Evaluation Set: Actual vs. Predicted Age')\n","plt.grid(True, linestyle='--', linewidth=0.5)\n","plt.show()\n","\n","# Step 5: Compute and print metrics\n","mse = np.mean((predicted_age_df[\"ActualAge\"] - predicted_age_df[\"predictedAge\"]) ** 2)\n","mae = np.mean(np.abs(predicted_age_df[\"ActualAge\"] - predicted_age_df[\"predictedAge\"]))\n","print(f\"üìä Evaluation MSE (Tuned GB): {mse:.4f}\")\n","print(f\"üìä Evaluation MAE (Tuned GB): {mae:.4f}\")\n","\n","# Step 6: Show the prediction table\n","display(predicted_age_df.head())\n","\n","# Step 7: Plot histogram of predicted ages\n","plt.figure(figsize=(8, 5))\n","plt.hist(predicted_age_df[\"predictedAge\"], bins=30, edgecolor='black')\n","plt.xlabel(\"Predicted Age\")\n","plt.ylabel(\"Count\")\n","plt.title(\"Distribution of Predicted Ages - Evaluation Set (Tuned GB)\")\n","plt.grid(True, linestyle='--', linewidth=0.5)\n","plt.show()\n","\n","# Step 8: Save results\n","eval_output_path = os.path.join(output_dir, \"EvaluationSet_Predicted_vs_Actual_TunedGB.csv\")\n","predicted_age_df.to_csv(eval_output_path)\n","print(f\"‚úÖ Evaluation predictions saved to: {eval_output_path}\")"],"metadata":{"id":"K3HiPs2SkUzY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NSxcG-DwA-YP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","# üßÆ Recompute final metrics\n","mse_final = np.mean((predicted_age_df[\"ActualAge\"] - predicted_age_df[\"predictedAge\"]) ** 2)\n","mae_final = np.mean(np.abs(predicted_age_df[\"ActualAge\"] - predicted_age_df[\"predictedAge\"]))\n","\n","# üìâ Print final evaluation metrics\n","print(f\"üìâ Final Evaluation MSE (Tuned GB): {mse_final:.4f}\")\n","print(f\"üìä Final Evaluation MAE (Tuned GB): {mae_final:.4f}\")\n","\n","# üìã Display predicted vs actual table\n","predicted_age_df.index.name = 'sampleId'\n","display(predicted_age_df.head(10))  # Top 10 preview\n","\n","# üìà Histogram of predicted ages\n","plt.figure(figsize=(8, 5))\n","plt.hist(predicted_age_df[\"predictedAge\"], bins=30, edgecolor='black')\n","plt.xlabel(\"Predicted Age\")\n","plt.ylabel(\"Count\")\n","plt.title(\"Distribution of Predicted Ages - Evaluation Set (Tuned GB)\")\n","plt.grid(True, linestyle='--', linewidth=0.5)\n","plt.tight_layout()\n","plt.show()\n","\n","# üíæ Save predictions table to CSV\n","eval_output_path = os.path.join(output_dir, \"Final_EvaluationSet_Predicted_vs_Actual_TunedGB.csv\")\n","predicted_age_df.to_csv(eval_output_path)\n","print(f\"‚úÖ Final prediction table saved to: {eval_output_path}\")\n"],"metadata":{"id":"MDSm6a69kXNA"},"execution_count":null,"outputs":[]}]}